1.自瞄系统逻辑设计

由于事先对自瞄这块了解不多，所以我先上了csdn找别人的开源项目，然后终于在一大堆源码（基本不讲思路）的文档中发现了吉大在github中开源项目的传送门，通过阅读readme文档，我对他们的视觉方案有了一定了解，然后就以借鉴他们的思路为主的同时填充了一点点我从别的文档看到的一些可能需要的小细节

2.每帧只框出一个虚拟装甲板

return写for循环里面了。。。

3.初次使用solvepnp，通过gpt知道输入包含参考尺寸，图像坐标，相机内参，畸变参数，输出是目标相对于相机的位置（旋转向量和平移向量，这个平移向量就是我们要的目标在相机坐标系中的3d坐标），在这里我对参考尺寸的坐标系选取产生疑惑（gpt用z=0），我想这装甲板一般是立起来的（是不是该用y=0），gpt就告诉我输出确实不一样，但是实际上只是坐标轴（用z=0得到的z最后是向前的）不同，于是我到csdn上查找机器人常用的坐标系，最后选择使用基坐标系（前x，左y，上z），然后gpt告诉我相机坐标系默认是x右, y下, z前，只需要再进行一次矩阵变换（交换xyz的方向）就能得到我想要的坐标系（这里还是以相机作为坐标系原点）

4.有时候会多识别很多装甲板（闪过去的编号甚至达到了4）

增加了一个判断，过滤掉面积小的轮廓防止误识别

5.能量机关识别，github源码太多英文不好，不方便直接找到相应的源码，于是到scdn上找,随后大概有了思路之后开始尝试，总之主要是参照里面的文档来的

然而第一步的二值化就弄半天没找到合适的上下限（随机截图取帧后通过滑块调节，但是取了几个不同的帧找到合适上下限之后用在视频的效果不是很理想），于是再次开始找源码借鉴其他思路

最后是以通道相减法二值化后的`mask_diff` 与hsv上下限得到的mask_hsv`（只取比较亮的部分）取交集勉强得到一个能看的轮廓。

关于待打击扇叶，根据文档所言，我选取了没有父轮廓和子轮廓的轮廓中面积最大的那个作为待打击扇叶，根据可视化结果基本上差不多，可是由于预处理没做好，导致他的轮廓有时会跟别的连到一起

关于待打击位置，我是按照将待打击扇叶几何中心与R的连线反向延长取去找的靶子的中心，其中R是用没有父轮廓和子轮廓的轮廓中面积最小的那个。开始的时候我是将R固定在第一帧出现的位置，发现R的位置有变化之后我尝试去动态追踪R，但是效果不好（R有时候会乱动甚至跑到扇叶上，我认为还是预处理的问题），最后就将他固定在第一帧的位置了。另外待打击位置乱动的原因除了R的位置不准以外，还有上述（有的轮廓会连在一起）这个原因

判断是否处于激活状态我是通过检测还有没有待击扇叶来判断的

最后整完了大概之后再次尝试调节hsv，经过半小时努力后优化了一点点，勉强能中3个了，于是觉得应该差不多了

目前感觉传统视觉的问题主要在预处理，要是预处理能做好大概也就没有深度学习的事了（要说算法，虽然糙了一些但大概是够用的，但是这个预处理我只能说是尽力了）

有想过是不是能用深度学习试一下，但是没有数据集而且想早点完成遂放弃